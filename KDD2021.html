<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>KDD2021</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="kdd2021報告">KDD2021報告</h1>
<h2 id="are-we-really-making-much-progress-revisiting-benchmarking-and-refining-heterogeneous-graph-neural-networks">Are we really making much progress?: Revisiting, benchmarking and refining heterogeneous graph neural networks</h2>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/3447548.3467350">論文</a></li>
</ul>
<h3 id="概要">概要</h3>
<p>Heterogeneous Graph Neural Network (HGNN) のベンチマークデータセットである <a href="https://github.com/THUDM/HGB">Heterogeneous Graph Benchmark (HGB)</a> を整備した。</p>
<p>このレポジトリで、3つのタスクの、全部で11のデータセットを使い、 これまで公開された複数の手法を使って精度比較をした報告</p>
<p>Are we really making much progress? はいわずもがな MaurizioFD氏による Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches が念頭にあり、Graph Neural Network の最近の進展を批判的に 取り上げた論文である。</p>
<p>様々な手法が最近提案されているが、これまでの手法の中で 最も精度が高かったのは、ほとんどがGATという初期の方法のケースで、 さらにGATを若干拡張した手法である、Simple-HGNという（単純な）手法が 最も精度が高かったという報告となっている。</p>
<h2 id="learning-based-proximity-matrix-factorization-for-node-embedding">Learning Based Proximity Matrix Factorization for Node Embedding</h2>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/3447548.3467296">論文</a></li>
<li>実装がない。</li>
</ul>
<h3 id="概要-1">概要</h3>
<p>Node Embedding の算出方法の提案。Lameneという名前。 Personalized PageRank におけるスタートに戻る確率を、各時刻において 一律でない値を採用し、これを学習するもの。 学習には、Differentiable SVD を採用する。PyTorchを利用。 ただし計算コストが高く、コストがかからないような工夫があるため、 ロジックが若干ややこしい。</p>
<p>計算した結果得られる Node Embedding による各タスクの精度は 他の手法よりも高い。既存の手法で最も高いものとして STRAP という 手法があるらしい。</p>
<h2 id="signed-graph-neural-network-with-latent-groups">Signed Graph Neural Network with Latent Groups</h2>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/3447548.3467355">論文</a></li>
<li><a href="https://github.com/haoxin1998/GS-GNN">実装</a> … まだ公開されていない感じ。</li>
</ul>
<h3 id="概要-2">概要</h3>
<p>敵・味方のネットワークで、node representation を得ようという話。 「味方の味方は味方」「敵の敵は味方」となっているように友達関係が 構成されるという “balance theory” (バランス理論) に基づいて愚直に representation を構成しようとすると破綻するので、ある程度グループ にまとめてしまい、グループ内で representation が似ていて、 そこから敵・味方関係の矛盾が少なくなるように representation を 算出するアルゴリズム GS-GNN を構成した。</p>
<h2 id="approximate-graph-propagation">Approximate Graph Propagation</h2>
<ul>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/3447548.3467243">論文</a></li>
<li><a href="https://github.com/wanghzccls/">著者github</a> … ただし実装がない</li>
</ul>
<h3 id="概要-3">概要</h3>
<p>Graph Propagation に基づく Node Representation の手法として、 Personalized PageRank (PPR), heat kernel, Katz などの手法があるが これらを統合したうえで、さらに近似計算で高速化した改良手法の提案</p>
<p>各種手法との精度・計算時間の比較グラフを見ると、 既存手法より10倍前後程度速くて、かつ既存手法より若干精度が劣る 程度にとどめられるとのこと。</p>
<h2 id="maximizing-influence-of-leaders-in-social-networks">Maximizing Influence of Leaders in Social Networks</h2>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/3447548.3467229">論文</a></li>
<li><a href="https://github.com/kedges/kedges">実装</a></li>
</ul>
<h3 id="概要-4">概要</h3>
<p>二極化した世論(0/1)で、人々の意見は身近な知り合いの意見の平均で 決まる、とするネットワーク上のダイナミクスモデル（これを一般的に Leader-Follower DeGroot Modelというらしい）が与えられたときに、 初期条件として1の意見をもつ人達(leaders)から、0の意見を持つ人たち (follower)に対して、任意にedgeをつないでいい時に、その本数が固定の 場合に、最もダイナミクスを無限回回した時の1の意見を持つ人の割合 が最大になるようなedgeの結び方の提案。</p>
<p>このようなedgeの選び方は submodular 性をもつので、greedy に選択 しても、ある程度精度が担保でき、かつ、1本追加で選んだ時の 1の割合は、逆行列を使った更新式 (Sherman-Morrison formula) で 更新できる。</p>
<p>Julia を使った実装が存在する。</p>
<h2 id="minirocket-a-very-fast-almost-deterministic-transform-for-time-series-classification">MiniRocket: A Very Fast (Almost) Deterministic Transform for Time Series Classification</h2>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/3447548.3467231">論文</a></li>
<li><a href="https://github.com/angus924/minirocket">実装</a></li>
</ul>
<h3 id="概要-5">概要</h3>
<p>時系列に対する判別モデル。 Convolution Kernel の作り方がうまくて、いろんなパターンのKernelを 長さを引き延ばしてたくさん作る。 先行する手法の Rocket と似ているが、乱数を使わない決定的手法。 Rocketと比較して精度を維持しつつ75倍速くなったといっている。</p>
<p>KDDに先行してすでに実装があったらしい。 sklearnと親和性が高く見える。</p>
<h2 id="a-broader-picture-of-random-walk-based-graph-embedding">A Broader Picture of Random-walk Based Graph Embedding</h2>
<ul>
<li><a href="https://dl.acm.org/doi/abs/10.1145/3447548.3467300">論文</a></li>
<li><a href="https://github.com/zexihuang/random-walk-embedding">実装</a></li>
</ul>
<h3 id="概要-6">概要</h3>
<p>これまでに提案されてきた Random-walk を用いる Graph Embedding 手法を 次の3つの要素で特徴づけて分類:</p>
<ul>
<li><ol type="1">
<li>遷移確率の取り方(連結要素だけに行くか, PageRankのようにジャンプがあるか)</li>
</ol></li>
<li><ol start="2" type="1">
<li>ノード間の類似度の計算の仕方(PMI or Autocovariance)</li>
</ol></li>
<li><ol start="3" type="1">
<li>ノードembeddingの算出方法(Matrix Factorization or Sampling)</li>
</ol></li>
</ul>
<p>これらの要素の組み合わせでまだ検証していない組み合わせについて 複数のデータセット・タスクで検証を改めて実施。</p>
<p>実際には、タスクごとに適した組み合わせがあり、例えば、 - PMI vs Autocovariance では、 - Node Classification では、PMIが性能高い - Link Prediction では、AutoCovarianceが性能高い その他、3つの要素いずれについても、タスクごとにどちらが優れて いるかはタスクによって変わるので、タスクによってそれぞれの要素 のどれがいいか選択するべきである、という趣旨らしい。</p>
<p>Graph Embedding については、上の “Approximate Graph Propagation” と同じく、手法がかなり発散していて、このような整理学が求められて いるという雰囲気がある。</p>
<h2 id="where-are-we-in-embedding-spaces-a-comprehensive-analysis-on-network-embedding-approaches-for-recommender-systems">Where are we in embedding spaces? A Comprehensive Analysis on Network Embedding Approaches for Recommender Systems</h2>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/3447548.3467421">論文</a>,<a href="https://arxiv.org/abs/2105.08908">arxiv</a></li>
<li><a href="https://github.com/RinneSz/Social-Collaborative-Metric-Learning">実装</a></li>
</ul>
<h3 id="概要-7">概要</h3>
<p>レコメンドタスクでHyperbolic embeddingがどれだけ有効かを ちゃんと調べてみようという研究。practitionerは、なんでも hyperbolic化したがるけど、そうなんでもhyperbolicにしたからと いってよくなるわけではないよという前置きから始まる。</p>
<p>3つの仮定を置き、実際のデータで仮説が正しいことを確認している。</p>
<ul>
<li><ol type="1">
<li>inner product based の手法より distance based の手法のほうが改善が大きい。</li>
</ol></li>
<li><ol start="2" type="1">
<li>データの密度が低い場合に hyperbolic 化での改善が大きい。</li>
</ol></li>
<li><ol start="3" type="1">
<li>hyperbolic の場合は、しない場合と比較して次元が低いほうが改善が大きい。</li>
</ol></li>
</ul>
<p>「データの密度が低い」は、行列密度が、0.01%-0.1%程度のスケールを指している。 MovieLensは100k-1Mは、4-6%程度であり、密度が高いケースになる。</p>
<p>実際にSocial Network で、これらの仮定が当てはまるような CMLの拡張手法 SCML と hyperbolic 化した HSCMF を提案し 確かに従来法よりも精度が改善することを確認。 ただし計算コストが高いので、注意したほうがいいというコメント。</p>
<p>レコメンドタスクでは、hyperbolic化により精度改善ができるケースが 比較的限られているので注意したほうがいいという指摘に見えた。</p>
<h2 id="news">News</h2>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/3447548.3467392">論文</a></li>
<li>実装はない。
<ul>
<li>first author である Jialu Liu の <a href="https://github.com/remenberl">github</a>にも実装は見当たらない。</li>
</ul></li>
</ul>
<h3 id="概要-8">概要</h3>
<p>News記事のembeddingを算出する話であるが、様々な工夫がある。</p>
<ul>
<li>事前学習済みのtransformerモデルを用いること</li>
<li>news記事1つに対してまったく同じだが表現が違うもの(positive)と よく似ているが、まったく別の記事(negative)でtripletを構成し、 contrastive learning で embedding をする。</li>
<li>この学習において、news記事に対して付与されている 記事カテゴリの判別タスクも含めて学習を実施し、 multitask learning をして精度を高めている。</li>
</ul>
<p>単にcontrastive learning するだけではなく、 multitaskにして精度を高めたりするあたりの工夫が光る。</p>
<p>またデータをクロールする際に、記事の時刻と、 記事についた画像のembeddingsにより、記事の同一性を担保するなど、 データの集め方やラベルのつけ方が素晴らしい。パイプラインの構成など、 学習システムの構築など非常に鮮やかな感じがする。</p>
<h2 id="triplet-attention-rethinking-the-similarity-in-transformers">Triplet Attention: Rethinking the similarity in Transformers</h2>
<ul>
<li><a href="https://dl.acm.org/doi/abs/10.1145/3447548.3467241">論文</a></li>
<li><a href="https://github.com/zhouhaoyi/TripletAttention">実装</a> … これを書いた時点では <em>We will release the code soon.</em> となっている</li>
</ul>
<h3 id="概要-9">概要</h3>
<p>Transformers モデルの Self-Attention の部分で、 Query,Key,Value のKeyの部分にKey1,Key2と拡張する。 この際に、Key1,Key2のOuter-productとQueryとのinner productが 大きい場合に値が大きいとする考え方。このようにすると文脈ごとに 意味が異なる表現を取り込めるという話らしい。</p>
<p><img src="./images/KDD2021/Triplet_Attention.png" /></p>
<p>ただしこの機構を取り入れて改善できた差分はあまりそこまで 大きくないし、定性的に何か新しいことが抽出できるという わけでもなさそうなので、あまりインパクトがないのではという印象。</p>
<h2 id="efficient-data-specific-model-search-for-collaborative-filtering">Efficient Data-specific Model Search for Collaborative Filtering</h2>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/3447548.3467399">論文</a></li>
<li>実装はみあたらない。</li>
</ul>
<h3 id="概要-10">概要</h3>
<p>Collaborative Filteringを改良した手法は、Embeddingの算出や inner productの算出部分に改良が加わっているという違いがあるだけ なので、これらを適用する/しないをデータごとにパラメータチューニング することで、データごとに特化した最適手法を抽出できるという前提で 組まれたレコメンド向け自動モデルチューニング機構についての報告。</p>
<p><img src="./images/KDD2021/Efficient_RS.png" /></p>
<p>心躍るタイトルで読み始めたのだが、前提として、Xiangnan He の <a href="https://arxiv.org/abs/1708.05031">Neural Collaborative Filtering</a> の精度が高いことを期待しているのだが、残念ながら NCF は Steffen Rendle さんが<a href="https://dl.acm.org/doi/10.1145/3383313.3412488">RecSys2020で、シンプルなMatrix Factorizationより精度が低い</a> というかなり辛らつだが確からしい実験報告が存在するあるので、この手の論文も Matrix Factorization 系のモデルについてチューニングが正しくされているか 怪しい感じがある。コンセプトは理解するが結果については信憑性に欠くのでは というのが素朴な印象。</p>
<h2 id="deconfounded-recommendation-for-alleviating-bias-amplification">Deconfounded Recommendation for Alleviating Bias Amplification</h2>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/3447548.3467249">論文</a></li>
<li><a href="https://github.com/WenjieWWJ/DecRS">実装</a></li>
</ul>
<h3 id="概要-11">概要</h3>
<p>レコメンドにおけるPopularity Biasの除外を因果推論の立場から考えた研究。 手法として、Factorization Machineとその亜種の改善が念頭にある。</p>
<p>User Embedding <span class="math inline">\(U\)</span> とUser Response <span class="math inline">\(Y\)</span> の両方に対して、 Item Groupへの直近の接触の多さという交絡因子が存在するため、 Popularity Biasが発生するという因果を仮定するところが出発点。</p>
<p>ここから、User Embedding <span class="math inline">\(U\)</span> や、Item Embedding <span class="math inline">\(I\)</span> とは独立に 「Item Group滞在分布」という変数 <span class="math inline">\(D\)</span> を FM モデルに入れて、 交絡因子の無効化(Deconfounding)を実施する DecRS を提案。 実際には、Item GroupごとのEmbedding <span class="math inline">\(M\)</span> を作って、<span class="math inline">\(U,I,M\)</span>の 3つ組でモデルの中に組み込むという形をとる。</p>
<p><img src="./images/KDD2021/Deconfounded.png" /></p>
<p>交絡因子の仮定までは理解だが、その後の取り扱い方が因果推論系では あまり一般的なやり方でないように見えるので、これで交絡因子を考慮した 因果推論として成立しているのかがよくわからない。手法を適用した後の 効果推定が通常のnDCGになっているのも、これで妥当なのかが疑問。 ただしシンプルなので、実運用には使いやすそう。</p>
<p>効果が表れやすいuserとして、Item Groupの季節変動が著しいuserへの 効果が表れやすいという話が面白い指摘に思った。 (レコメンド結果で行動に対してバイアスがかかりやすい人ほど popularity bias補正の効果が出やすいという指摘)</p>
<p>Xiangnan He さんはこの方向性に舵を切ってきたらしい。。</p>
<h2 id="timeshap-explaining-recurrent-models-through-sequence-perturbations">TimeSHAP: Explaining Recurrent Models through Sequence Perturbations</h2>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/3447548.3467166">論文</a></li>
<li>実装は見当たらない</li>
</ul>
<h3 id="概要-12">概要</h3>
<p>SHAPは、回帰/判別モデルでの説明変数の重要度の算出に用いる手法で、 LIMEと並んでよくつかわれるが、計算オーダーが変数の数の指数オーダーに なる点が懸念点であるため、時系列イベントデータに使うにはコストが高い。</p>
<p>そこで、TimeSHAPと呼ばれる次の工夫を入れた。 (SHAPはそのままに、SHAPに入力するデータの入れ方の工夫だけにみえる)</p>
<ol type="1">
<li>時系列データを現在から一定時刻前と後で2分割して扱ってSHAPで shapley値を計算し、最も効果の高い分割を見つける。 (過去時刻の探索を効率化するpruning)</li>
<li>決定した時刻より前の変数について、時刻ごと/変数ごとにグループ分けし 最も効果のある組み合わせを見つける（時刻x変数の全組み合わせを 探すと破綻するので。図）</li>
</ol>
<p><img src="./images/KDD2021/TimeSHAP.png" /></p>
<p>この手順を、実際の（おそらくポルトガルに拠点がある）銀行のトランザクションの GRUによる（おそらく異常利用の）判別モデルに適用し、どの時刻のどの特徴量が 判別に影響があるかを算出できたといっている。</p>
<p>ADS Track (Industrial Track相当)での発表なので、 実業務への適用実績に力点があり、技術的に新しい何かがある報告ではないが、 比較的シンプルな工夫でうまく効果を出せているように見える。</p>
<h2 id="amazon-sagemaker-clarify-machine-learning-bias-detection-and-explainability-in-the-cloud">Amazon SageMaker Clarify: Machine Learning Bias Detection and Explainability in the Cloud</h2>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/3447548.3467177">論文</a></li>
<li><a href="https://aws.amazon.com/jp/sagemaker/clarify/">Amazon SageMaker Clarify のページ</a></li>
</ul>
<h3 id="概要-13">概要</h3>
<p>Amazon SageMakerにdeployしたモデルと、学習に用いたデータセットを使って、 モデルに含まれる (バイアスがかかりがちな事前指定した特定の属性に対する) バイアスの定量化と、予測根拠となる説明結果を与える Amazon SageMaker Clarify というサービスについての紹介。</p>
<p>学習データ自体の偏り(Pre-Training Metrics)と、学習後のモデルが出力する 予測結果が持つ偏り(Post-Training Metrics)を測定、 Pre-Training ではCI/DPL/CDDL, Post-Training では DPPL/DI/DCA/AD/RD/DAR/TE/CDDPI といった指標が使われるらしい（割と単純な計算式で与えられるものが多い）</p>
<p>予測結果の説明ではKernelSHAPが使われるとのこと。</p>
<p>いずれもこれと言って目新しいものはないが、deployしたモデルの モニタリング目的で利用できるサービスであるという主張。</p>
<h2 id="a-hyper-surface-arrangement-model-of-ranking-distributions">A hyper-surface arrangement model of ranking distributions</h2>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/3447548.3467253">論文</a></li>
<li><a href="https://github.com/shizuo-kaji/rankLearning">実装</a> : Chainerを使った実装になっている。</li>
</ul>
<h3 id="概要-14">概要</h3>
<p>九大の鍛冶静雄さんのグループの発表。</p>
<p>アイテムのランキングデータが与えられているときに、このランキングに 最も適合するような多次元空間上の埋め込みを与えるようなアルゴリズムを 考えたという話。Plackett-Luceなどのように1変量にする前提のものは多いが、 ここでは多次元の埋め込みを得られることが利点。</p>
<p>ランキング対象となるアイテム1つが空間上の1点に対応していて、 かつ、空間上の1点がランキング1つに対応しており、その点に近い順に ランキングが生成されると考える。そのため、アイテムに対応する点で 空間をVoronoi分割すると、Voronoi分割された領域で同一のランキングが 生成される理屈となっている。</p>
<p>神嶌さんの<a href="https://www.kamishima.net/sushi/">寿司データセット</a>に対して適用した結果が報告されていて、 マグロ系の寿司が概ね同じ地位を占めたり、かっぱ巻きなどが遠くに位置するなど、 概ねデータの全体の傾向を表すような埋め込みベクトルを得ることができるとのこと。</p>
<p><img src="./images/KDD2021/HyperSurfaceArrangement.png" /></p>
<p>数値検証では、ランキングの上位部分観測から、下位ランキングの推定などが できるという報告があり、ランキングデータが与えられている場合のレコメンド などに使えるのではと思った（どういう業種であればランキングデータがあるのかな…）</p>
<h2 id="debiasing-learning-based-cross-domain-recommendation">Debiasing Learning based Cross-domain Recommendation</h2>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/3447548.3467067">論文</a></li>
<li>実装はない。</li>
</ul>
<p>Cross-domain Recommendation といった場合、ユーザが共通する 全く別のデータセットでのレコメンドにおいて、共通ユーザの情報を 互いに使いあうことで精度を高めあうという意味合いが強い。</p>
<p>しかし利用者がオーバーラップするような、2つのネットショッピングサイトで、 まったく同じユーザが、それぞれのサイトで全く別の行動をとることが実際にあり、 これをどのように補正するか？という中国のTaobaoからの報告。</p>
<p>因果グラフにおいて、ドメインに特化した交絡因子(DSC)、共通の交絡因子(GC) を考える。DSCとして考えられるのが、サイトの性質を表す変数であり、 これによって user embeddings(<span class="math inline">\(\boldsymbol{v}_u\)</span>) も、購入(<span class="math inline">\(y\)</span>)も影響を 受けると考えて IPS (傾向スコア逆数スコアリング) でレコメンドの損失に 重みづけをして学習するなどし、バイアス補正の形でドメイン間でのユーザ行動の変化を 吸収したという話。その他いくつかの工夫がある。 評価においては、FM/Wide&amp;Deep/DeepFMに対して提案法を適用し、 従来の Cross-domain Recommendation 手法より精度改善したとのこと。</p>
<p><img src="./images/KDD2021/DebiasingCrossDomainRecSys.png" /></p>
<p>本当にうまくいったのであれば素晴らしいが、 この結果を出すまでに因果を整理できるかが肝なのだろうと思われ、 実践は難しいだろうなという印象を持った。</p>
<h1 id="learning-to-embed-categorical-features-without-embedding-tables-for-recommendation">Learning to Embed Categorical Features without Embedding Tables for Recommendation</h1>
<ul>
<li><a href="https://virtual.2021.kdd.org/paper_Research_Track-92.html">論文</a></li>
<li>実装はない。</li>
</ul>
<p>Recommendationにおいて、User/Itemの数が非常に多い場合、 単純なMatrix FactorizationでもEmbedidngテーブルが巨大になる。 これを回避するための方法としてハッシュ関数を使う方法が近年 多数提案されているが collision 問題が解決されていない。</p>
<p>これを解決するために、Deep Hash Embedding (DHE) という 方法を考えた。これはUser ID/Item ID (巨大な整数を想定) から1000個くらいのハッシュ化された(小さな範囲の)整数を生成し、 これらを単に実数の範囲に正規化する。この正規化されたベクトルを、 多層Denseを通してEmbeddingを作り、ここからRecommendationモデルを （Denseの学習も併せて）学習するという流れ。</p>
<p><img src="./images/KDD2021/DeepHashEmbedding.png" /></p>
<p>パラメータ数で比較すると、他の手法に比べて精度が高いと言っているのが 本当だとすると、アイデアがシンプルにもかかわらずいい結果を出していて なかなか素晴らしいのだが、容易に想像できるように計算コストが高く、 素のEmbeddingと比べて10倍以上時間がかかると言っている。 （GPUでboostできると言っているが、この点は苦しいと言わざるを得ない）</p>
<p>というわけで、研究としては生煮え感がぬぐえないが、他のfeature を混ぜ込むことが容易であるので、このアイデアをベースにした 改良手法がやってくるのではという点で、今後が期待される。</p>
<h2 id="discrete-choice-models-with-interpretable-context-effects">Discrete Choice Models with Interpretable Context Effects</h2>
<ul>
<li><a href="https://dl.acm.org/doi/10.1145/3447548.3467250">論文</a></li>
<li><a href="https://github.com/tomlinsonk/feature-context-effects">実装</a></li>
</ul>
<h3 id="概要-15">概要</h3>
<p>離散選択モデル(Discrete Choice Model)では、最もシンプルには 多項ロジットを使うモデリングが行われるのだが、仮定として IIA(independence of irrelevant alternatives ; 無関係な選択肢からの独立性) が成り立つ必要がある((wikipedia)[https://en.wikipedia.org/wiki/Independence_of_irrelevant_alternatives])が 一般にはこの仮定は成り立たない。例えば、3つの商品があったときに 常に真ん中ぐらいの商品が選択されやすいとかいう現象が起きたりする。 (compromiseというらしい)</p>
<p>このような、他の選択肢の傾向を context として変数に組み込み context を考慮したモデルは以前にも提案されてきたが、今回の提案は、 これを Linear で入れたモデル(Linear Context Logit ; LCL)と、 およびそれを線形混合したモデル(Decomposed Linear Context Logit ; CLCL) とする。基本的にitem featureとcoefficientの線形モデルなのだが、 item featureの平均ベクトルを context として入れ、coefficientに対する contextの影響を行列として入れるというモデル。</p>
<p><img src="./images/KDD2021/LinearContextLogit.png" /></p>
<p>離散選択モデル向けのベンチマークデータセットと、グラフにおける 隣接ノード選択問題（この場合は、エッジを構成した時の三角関係の数とか、 InEdge数とかがfeatureとなる）に適用した精度が従来法よりも高いとの指摘。 実装もある。</p>
<p>離散選択モデルの話は古くからある話題で、このような線形モデルで Contextを入れるという改良が今までなかったのが不思議なくらいにシンプル だが、しっかり精度を上げてきている真面目な研究である。</p>
</body>
</html>
